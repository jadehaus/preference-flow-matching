{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Conditional Image Generation with MNIST Dataset\n",
    "\n",
    "In this example, we demonstrate how PFM can be used to modify and improve the output sample from the reference image generation model, with the classical MNIST image dataset. \n",
    "\n",
    "We utilize a pre-trained DCGAN generator as the reference policy, $\\pi_{\\mathrm{ref}}$, to generate sample pairs conditioned on digit labels $x \\in {0, \\dots, 9}$. To create preference datasets, we assign preferences to these pairs based on the softmax probabilities of the labels predicted by a LeNet classifier. Subsequently, we learn a PFM flow $v_{\\theta}$ to map less preferred samples $y^{-}$ to more preferred samples $y^{+}$ under a given condition $x$.\n",
    "\n",
    "You can use any pre-trained generator or preference model of your choice for this task. For convenience, we provide pre-trained versions of both the DCGAN generator and the LeNet classifier. The weight parameters are available in `./models/weights/`. Alternatively, you can train your own generative model by running `./models/mnist_generator.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from models import Generator, LeNet5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = Generator()\n",
    "generator.load_state_dict(\n",
    "    torch.load(\"./models/weights/generator.pth\", map_location=device)\n",
    ")\n",
    "classifier = LeNet5()\n",
    "classifier.load_state_dict(\n",
    "    torch.load(\"./models/weights/classifier.pth\", map_location=device)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preference Dataset Collection\n",
    "\n",
    "Using the obtained generator and a preference (reward) model, you can construct your own preference dataset. You can use the below sample codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import generate_mnist_dataset, RewardFunction\n",
    "\n",
    "reward_function = RewardFunction(classifier, device=device)\n",
    "dataset = generate_mnist_dataset(generator, reward_function, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training PFM on MNIST Preference Dataset\n",
    "\n",
    "Training a flow matching module can be done within a few lines of codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow import OptimalTransportConditionalFlowMatching\n",
    "from models import UNet\n",
    "\n",
    "flow_model = UNet(\n",
    "    dim=(1, 28, 28),\n",
    "    class_cond=True,\n",
    "    num_classes=10,\n",
    ").to(device)\n",
    "flow_matching = OptimalTransportConditionalFlowMatching(flow_model, device=device)\n",
    "\n",
    "trained_model, _ = flow_matching.fit(\n",
    "    dataset,\n",
    "    num_epochs=100,\n",
    "    batch_size=125,\n",
    "    learning_rate=1e-3,\n",
    "    conditional=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Transported Output Samples\n",
    "\n",
    "Once the PFM module is trained, it can directly be attached to the generator to adjust the output samples to be more alinged to the preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.arange(10, device=device).repeat(10)\n",
    "source = generator.sample(\n",
    "    num_samples=100,\n",
    "    labels=labels\n",
    ")\n",
    "grid = make_grid(\n",
    "    source.view([-1, 1, 28, 28]).clip(-1, 1), value_range=(-1, 1), padding=0, nrow=10\n",
    ")\n",
    "img = ToPILImage()(grid)\n",
    "plt.imshow(grid[0, :, :].cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply apply PFM to the source (generated from the refernce policy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = source.view([-1, 1, 28, 28])\n",
    "target = flow_matching.compute_target(\n",
    "    source.to(device), \n",
    "    context=labels\n",
    ")\n",
    "grid = make_grid(\n",
    "    target.view([-1, 1, 28, 28]).clip(-1, 1), value_range=(-1, 1), padding=0, nrow=10\n",
    ")\n",
    "img = ToPILImage()(grid)\n",
    "plt.imshow(grid[0, :, :].cpu().detach().numpy())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
