{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Aligning RL Policies with PFM\n",
    "\n",
    "In this example, we demonstrate how PFM can enhance action planning. To illustrate our approach, we use the simple `BipedalWalker` environmentâ€”a physics-based simulation game built on box2d.\n",
    "\n",
    "Aligning reinforcement learning policies presents a unique challenge: preference datasets typically consist of trajectory pairs, requiring alignment methods to extract a preference signal from these samples. The policy is then adjusted to improve action planning based on this learned preference.\n",
    "\n",
    "We adopt a pre-trained policy provided by `Stable-Baselines3` using `DDPG`. Try running the below cell to see how the pre-trained reference policy iteracts with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from huggingface_sb3 import load_from_hub\n",
    "from stable_baselines3 import DDPG\n",
    "\n",
    "from dataset import BipedalWalkerResetWrapper\n",
    "\n",
    "\n",
    "env = gym.make('BipedalWalker-v3', render_mode=\"rgb_array\")\n",
    "env = BipedalWalkerResetWrapper(env)\n",
    "\n",
    "checkpoint = load_from_hub(\n",
    "\trepo_id=\"sb3/ddpg-BipedalWalker-v3\",\n",
    "\tfilename=\"ddpg-BipedalWalker-v3.zip\",\n",
    ")\n",
    "policy = DDPG.load(checkpoint)\n",
    "\n",
    "done = False\n",
    "obs, _ = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action, _ = policy.predict(obs, deterministic=False)\n",
    "    obs, _, terminated, truncated, _ = env.step(action)\n",
    "    done = (terminated or truncated)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(env.render())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preference Dataset Collection\n",
    "\n",
    "Following the prior works on the preference-based reinforcement learning (PbRL) literature, we first randomly choose a starting state $s_{0} \\sim S$, and sample two trajectories $\\tau^{+}, \\tau^{-}$, where the preference $\\tau^{+} > \\tau^{-}$ is obtained using a scripted teacher. For simplicity, we define a sample \"jumping reward\" that motivates the agent to jump more while walking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import generate_pbrl_dataset\n",
    "\n",
    "\n",
    "def jump_reward(state):\n",
    "    \"\"\"\n",
    "    Sample reward function to use as a scripted teacher.\n",
    "    You can replace this function with any other models of your interest.\n",
    "    \"\"\"\n",
    "    vel_y = state[3]\n",
    "    return abs(vel_y * 100) ** 2\n",
    "\n",
    "\n",
    "seg_len = 10        # segment length for action sequence planning and learning\n",
    "num_pairs = 1000    # number of sample pairs to collect\n",
    "\n",
    "dataset = generate_pbrl_dataset(\n",
    "    env, policy, jump_reward, \n",
    "    seg_len=seg_len,\n",
    "    num_pairs=num_pairs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training PFM for RL Policy Alignment\n",
    "\n",
    "Since our dataset consists of trajectory-level preference pairs $(x, \\tau^{+}, \\tau^{-})$, where the context $x$ is a current state observation $s_{0}$, and\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tau^{+} &:= (a_{0}, a_{1}, \\cdots, a_{\\ell}) \\sim \\pi_{\\mathrm{ref}}(\\cdot | s_{0})\\\\\n",
    "\\tau^{-} &:= (a_{0}', a_{1}', \\cdots, a_{\\ell}') \\sim \\pi_{\\mathrm{ref}}(\\cdot | s_{0})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with fixed length $\\ell \\geq 2$ (denoted by `seg_len` in the code), we directly learn a flow among the action trajectories, from $\\tau^{-}$ to $\\tau^{+}$, conditioned on the initial state $s_{0}$. Hence, the input dimension of the flow matching module should be `seg_len`$\\times$`action_dim`. We use a simple multi-layer perceptron (MLP) for our flow matching module.\n",
    "\n",
    "Within this formulation, training a flow matching module can be done within a few lines of codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "import torch\n",
    "\n",
    "from flow import OptimalTransportConditionalFlowMatching\n",
    "from models import MLP\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_dim = env.action_space.shape[0] * seg_len\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "flow_model = MLP(input_dim, context_dim=state_dim).to(device)\n",
    "flow_matching = OptimalTransportConditionalFlowMatching(flow_model, device=device)\n",
    "\n",
    "trained_model, _ = flow_matching.fit(\n",
    "    dataset,\n",
    "    num_epochs=10,\n",
    "    batch_size=100,\n",
    "    learning_rate=1e-3,\n",
    "    conditional=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with RL Environment with PFM\n",
    "\n",
    "For inference at a given state $s_{t}$, we sample an action trajectory $\\tau = (a_{t}, \\cdots, a_{t+\\ell})$ from the reference policy $\\pi_{\\mathrm{ref}}(\\cdot|s_{t})$, and apply flow matching to obtain a better action sequence. Then, we choose for the first action of the obtained action sequence as our final action in the current state $s_{t}$. This process is done for every current state observation $s_{t}$, which requires an environment dynamics model to rollout a sample trajectory using a reference policy. Therefore, we provide the PFM policy with a copy of an environment, and a reference policy. All this process is wrapped within the `FlowPolicy` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from flow import FlowPolicy\n",
    "\n",
    "done = False\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# A wrapper policy class that improves action planning,\n",
    "# using trained flow matching module.\n",
    "flow_policy = FlowPolicy(env, flow_matching, policy, seg_len=seg_len)\n",
    "\n",
    "# Environment iteraction using PFM policy\n",
    "while not done:\n",
    "    action = flow_policy(obs, use_torchdiffeq=False)\n",
    "    obs, _, terminated, truncated, _ = env.step(action)\n",
    "    done = (terminated or truncated)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(env.render())\n",
    "    plt.show()\n",
    "    \n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
