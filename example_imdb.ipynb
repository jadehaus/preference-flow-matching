{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Text Generation with IMDB Dataset\n",
    "\n",
    "In this example, we demonstrate how PFM can be used to modify and improve the output sample from the reference text generation model, on a controlled (positive) sentiment review generation task. \n",
    "\n",
    "As done in the DPO paper, to perform a controlled evaluation, we adopt a pre-trained sentiment classifier as the preference annotator. In this example, we chose `lvwerra/distilbert-imdb` from the `huggingface` pipeline. We also adopt a pre-trained GPT-2 on the IMDB dataset (`lvwerra/gpt2-imdb`) from the `huggingface` library as our reference model $\\pi_{\\mathrm{ref}}$.\n",
    "\n",
    "For our PFM module to be able to modify variable-length text outputs, we employ a T5-based autoencoder (`thesephist/contra-bottleneck-t5-large-wikipedia`) to work with fixed-sized embeddings, and later decode them back to texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from dataset import load_imdb_dataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataset = load_imdb_dataset(\n",
    "    classifier_checkpoint=\"lvwerra/distilbert-imdb\",\n",
    "    pretrained_checkpoint=\"lvwerra/gpt2-imdb\",\n",
    "    autoencoder_checkpoint='thesephist/contra-bottleneck-t5-large-wikipedia',\n",
    "    split='train[:10000]', \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training PFM on IMDB Preference Dataset\n",
    "\n",
    "Training a flow matching module can be done within a few lines of codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "from flow import OptimalTransportConditionalFlowMatching\n",
    "from models import UNet\n",
    "\n",
    "flow_model = UNet().to(device)\n",
    "flow_matching = OptimalTransportConditionalFlowMatching(flow_model, device=device)\n",
    "\n",
    "trained_model, _ = flow_matching.fit(\n",
    "    dataset,\n",
    "    num_epochs=100,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-3,\n",
    "    conditional=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Transported Output Samples\n",
    "\n",
    "Once the PFM module is trained, it can directly be attached to the generator to adjust the output samples to be more alinged to the preference. \n",
    "\n",
    "For our reference model, we employ a pre-trained GPT-2 model on the IMDB dataset. Since our PFM module is trained using the fixed-sized latent embeddings from the T5-based autoencoder, we need an extra encoding and decoding steps to apply PFM. In particular, we apply a learned flow to the outputs encoded by the autoencoder, and then decode the modified outputs back to texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from models import BottleneckT5Autoencoder\n",
    "from dataset import IMDBEvaluator\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"lvwerra/gpt2-imdb\").to(device)\n",
    "\n",
    "evaluator = IMDBEvaluator(device=device)\n",
    "autoencoder = BottleneckT5Autoencoder(\n",
    "    model_path='thesephist/contra-bottleneck-t5-large-wikipedia', \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may try running the below codes multiple times to visualize the outputs from the base (reference) policy and PFM modified outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"The movie was\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "num_iter = 5\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 25,\n",
    "}\n",
    "\n",
    "outputs = model.generate(**inputs, **generation_kwargs) #.squeeze()[-TXT_OUT_LEN:]\n",
    "outputs = tokenizer.decode(outputs.squeeze(), skip_special_tokens=True)\n",
    "print(f\"Generated review from GPT-2 pretrained model:\\n{outputs}\")\n",
    "print(f\"Score: {evaluator(outputs)}\\n\")\n",
    "\n",
    "source = autoencoder.encode(outputs).view(1, 1, 32, 32)\n",
    "for _ in range(num_iter):\n",
    "    source = flow_matching.compute_target(source, context=None) \n",
    "\n",
    "target = autoencoder.decode(source.view(1, -1))\n",
    "print(f\"Improved review using PFM:\\n{target}\")\n",
    "print(f\"Score: {evaluator(target)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
